To-do:

o) Add a 'count' field to the hash and allow incrementing it to do collision
   overflow.  For this it'd be nice to have an interface that would return a
   range of matches in the dictionary.  Put the count at the end to make this
   possible.  Would need to change the encoding logic to use a different OP
   for these that took, say, a count or even just the full hash/identifier.
o) Only have N bytes outstanding at any given time (say 128k?) and add some
   type of ACK, perhaps?  This is necessary to:
o) Write a garbage-collector for the dictionary.  LRU?

Possibly-bad future ideas:

o) Incorporate run-length encoding.
o) Possibly use two hash functions -- even just mixing crc in with the Adler
   stuff might help increase our hash entropy?  Been a while since I really
   read up on the subject and would do well to do so again.  Currently the
   entropy is not great.
o) Incorporate occasional (figure out frequency) CRCs or such of the next N
   bytes of decoded data to make it possible to detect any hash mismatches,
   using a different hash function to any that go into the hash.
o) If the encoded version of a stream is larger than the source would be
   escaped, it'd be nice to just transmit it escaped and to have some way to
   tell the remote side how to pick out chunks to be taken as known to both
   parties in the future.  One approach would be to send a list of offsets
   at which hashes were declared.
